import pandas as pd
import causaldag as causal_dag
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator
from pgmpy.inference import VariableElimination
from pgmpy.factors.discrete import TabularCPD
from itertools import product
import pickle
import os
import re
import copy
from joblib import Parallel, delayed

def fitPgmpyDAG(DAG, data, method='MLE', ess=10):
    """
    Fit parameters to a pgmpy Bayesian Network using MLE or Bayesian estimation using a BDeu prior.

    Args:
        DAG (BayesianNetwork): The pgmpy DAG structure to be fitted. For DBNs, this should be the
            unrolled static representation.
        data (pd.DataFrame): DataFrame containing categorical training data.
        method (str): Estimation method. Use 'MLE' for Maximum Likelihood Estimation or
            'Bayesian' for Bayesian Estimation using a BDeu prior.
        ess (int, optional): Equivalent sample size for Bayesian estimation. Default is 10.

    Returns:
        BayesianNetwork: The input DAG with fitted CPDs based on the specified estimation method.
    """
    fittedDAG = copy.deepcopy(DAG)

    data.columns = [col.strip() for col in data.columns]
    model_nodes = set(fittedDAG.nodes())
    data_columns = set(data.columns)

    if not model_nodes.issubset(data_columns):
        missing_nodes = model_nodes - data_columns
        raise ValueError(f"Data missing columns for nodes: {missing_nodes}")

    for col in data.columns:
        data[col] = data[col].astype('category')

    if method == 'MLE':
        fittedDAG.fit(data, estimator=MaximumLikelihoodEstimator)
    elif method == 'Bayesian':
        fittedDAG.fit(data, estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=ess)
    else:
        raise ValueError("Invalid method specified. Use MLE or Bayesian.")

    return fittedDAG

def buildUnrolledDBN(data, parameter_estimation, edgeListFile=None, frozen_set=None):
    """
    Construct an unrolled static Bayesian Network across 3 time slices from a 2-TBN template.

    Builds an unrolled Bayesian Network representing time slices t, t-1, and t-2
    from a 2-TBN template structure. A fourth slice (t-3) is included
    to provide valid parent nodes for inter-slice dependencies.

    The template structure can be defined in one of two ways:
    - edgeListFile: Path to a CSV file with two columns, 'from' and 'to', listing directed edges.
    - frozen_set: A Python set of (from, to) edges representing a DAG. This option is included to support integration
      with the causaldag package, which returns DAGs from the I-equivalence class of a CPDAG as sets of directed edges.

    **Note:** This implementation is hard-coded to unroll 3 full time slices (t, t-1, t-2)
    with support for an additional t-3 layer used only for dependency resolution. It must be
    generalized to support a different number of time slices.

    Args:
        data (pd.DataFrame): Training data used to estimate CPDs for the template structure.
        parameter_estimation (str): Parameter estimation method. Either MLE or Bayesian
        edgeListFile (str, optional): Path to CSV edge list for the 2-TBN template structure.
        frozen_set (set of tuples, optional): A set of edges representing a DAG.

    Returns:
        BayesianNetwork: A pgmpy BayesianNetwork object containing the unrolled structure and fitted CPDs
        across 3 full time slices.
    """
    unrolled_time_slices = 4 # This thesis models 3 full time slices: t, t-1, and t-2.
                             # This value is set to 4 to also include t-3, which provides
                             # necessary parent values for certain inter-slice edges.

    # Build the DBN template structure from either a csv file with a list of edges or from a frozen set generated by
    # causaldag which generates DAGs from a CPDAG
    if edgeListFile:
        base_dir = os.path.dirname(__file__)
        edge_list_path = os.path.abspath(os.path.join(base_dir, "..", "data", "final_network", edgeListFile))
        edgesDf = pd.read_csv(edge_list_path)
        template_edges = list(edgesDf.itertuples(index=False, name=None))
        templateBn = BayesianNetwork(template_edges)
    elif frozen_set:
        template_edges = list(frozen_set)
        templateBn = BayesianNetwork(template_edges)
    else:
        raise ValueError("Either edgeListFile or frozen_set must be provided.")

    # Fit CPDs on template
    fitted_template_bn = fitPgmpyDAG(templateBn, data, method=parameter_estimation)
    templateCPDs = fitted_template_bn.get_cpds()

    unrolledDBN = BayesianNetwork()
    all_nodes = set()
    unrolled_edges = set()
    t3_nodes = set()

    # Identify intra-slice and inter-slice edges from the template
    intra_slice_template_edges = [(u, v) for u, v in template_edges if not u.endswith("_lag1")]
    inter_slice_template_edges = [(u.replace("_lag1", ""), v) for u, v in template_edges if u.endswith("_lag1")]

    # Identify t-3 nodes (only those that have edges into t-2). This is done by identifying nodes from t-1 that
    # influence t in the template
    for u, v in inter_slice_template_edges:
        t3_nodes.add(u)

    # Loop through each time slice (t-3 to t) to build unrolled structure
    for t in range(unrolled_time_slices):
        time_label = "" if t == 3 else f"_lag{3 - t}"

        # Add nodes for current time slice
        for node in templateBn.nodes():
            base_node = node.replace("_lag1", "")
            if t in [1, 2, 3] or (t == 0 and base_node in t3_nodes):
                unrolled_node = base_node + time_label
                all_nodes.add(unrolled_node)

        # Add intra-slice edges
        for u, v in intra_slice_template_edges:
            if t in [1, 2, 3]:
                unrolled_edges.add((u + time_label, v + time_label))

        # Add inter-slice edges
        if t < unrolled_time_slices - 1:
            next_time_label = "" if t == 2 else f"_lag{unrolled_time_slices - 1 - (t + 1)}"
            for from_node, to_node in inter_slice_template_edges:
                if from_node in t3_nodes or t < 3:
                    unrolled_edges.add(
                        (from_node + time_label, to_node + next_time_label)
                    )

    # Add all collected nodes and edges to the unrolled DBN
    unrolledDBN.add_nodes_from(all_nodes)
    unrolledDBN.add_edges_from(unrolled_edges)

    # Copy CPDs into each time slice
    for template_cpd in templateCPDs:
        # Helper to update evidence names based on which time slice is being populated
        def adjustEvidenceName(ev, time_suffix):
            if time_suffix == "":  # Target is t
                return ev if not ev.endswith("_lag1") else ev  # Keep _lag1 if it was _lag1

            elif time_suffix == "_lag1":  # Target is t-1
                return ev.replace("_lag1", "_lag2") if ev.endswith("_lag1") else ev + "_lag1"

            elif time_suffix == "_lag2":  # Target is t-2
                return ev.replace("_lag1", "_lag3") if ev.endswith("_lag1") else ev + "_lag2"

        var_name = template_cpd.variable
        evidence_vars = template_cpd.variables[1:]
        template_state_names = template_cpd.state_names if hasattr(template_cpd,
                                                                   "state_names") and template_cpd.state_names else {}

        cardinality = template_cpd.cardinality.tolist()
        variable_card = int(cardinality[0])
        evidence_card = list(cardinality[1:]) if len(cardinality) > 1 else None

        # Case 1: Variable originates from t-1 (needs a t-3 CPD only)
        if var_name.endswith("_lag1"):
            new_var_t3 = var_name.replace("_lag1", "_lag3")
            new_evidence_t3 = None
            state_names_t3 = {adjustEvidenceName(var, "_lag2"): states for var, states in template_state_names.items()} \
                if template_state_names else None

            CPD_t3 = TabularCPD(
                variable=new_var_t3,
                variable_card=variable_card,
                values=template_cpd.get_values(),
                evidence=new_evidence_t3,
                evidence_card=evidence_card,
                state_names=state_names_t3
            )
            unrolledDBN.add_cpds(CPD_t3)
        # Case 2: Variable exists in t, t-1, and t-2 (fully replicated across slices)
        else:
            # Time t
            new_var_t = var_name
            new_evidence_t = [adjustEvidenceName(ev, "") for ev in evidence_vars]
            state_names_t = {adjustEvidenceName(var, ""): states for var, states in template_state_names.items()} \
                if template_state_names else None

            CDP_t = TabularCPD(
                variable=new_var_t,
                variable_card=variable_card,
                values=template_cpd.get_values(),
                evidence=new_evidence_t if new_evidence_t else None,
                evidence_card=evidence_card,
                state_names=state_names_t
            )
            unrolledDBN.add_cpds(CDP_t)

            # Time t-1
            new_var_t1 = var_name + "_lag1"
            new_evidence_t1 = [adjustEvidenceName(ev, "_lag1") for ev in evidence_vars]
            state_names_t1 = {adjustEvidenceName(var, "_lag1"): states for var, states in template_state_names.items()} \
                if template_state_names else None

            CPD_t1 = TabularCPD(
                variable=new_var_t1,
                variable_card=variable_card,
                values=template_cpd.get_values(),
                evidence=new_evidence_t1 if new_evidence_t1 else None,
                evidence_card=evidence_card,
                state_names=state_names_t1
            )
            unrolledDBN.add_cpds(CPD_t1)

            # Time t-2
            new_var_t2 = var_name + "_lag2"
            new_evidence_t2 = [adjustEvidenceName(ev, "_lag2") for ev in evidence_vars]
            state_names_t2 = {adjustEvidenceName(var, "_lag2"): states for var, states in template_state_names.items()} \
                if template_state_names else None

            CPD_t2 = TabularCPD(
                variable=new_var_t2,
                variable_card=variable_card,
                values=template_cpd.get_values(),
                evidence=new_evidence_t2 if new_evidence_t2 else None,
                evidence_card=evidence_card,
                state_names=state_names_t2
            )
            unrolledDBN.add_cpds(CPD_t2)

    # Final check to ensure model is valid
    unrolledDBN.check_model()

    return unrolledDBN

def generateCPDAG(adj_matrix_filename):
    """
    Generate a CPDAG from a DAG adjacency matrix.

    Loads a DAG from a CSV-formatted adjacency matrix and converts it into its corresponding
    CPDAG using the causaldag package. The input DAG must be stored in adjacency matrix form, where
    rows and columns correspond to variables and a 1 indicates a directed edge from row variable to column variable.

    Args:
        adj_matrix_filename (str): The name of the CSV file (located in ../data/final_network/) containing
            the adjacency matrix of the DAG.

    Returns:
        causaldag.DAG: A CPDAG causaldag object
    """
    base_dir = os.path.dirname(__file__)
    file_path = os.path.abspath(os.path.join(base_dir, "..", "data", "final_network", "Final_Network_Adj_Matrix.csv"))
    DAG_adj_matrix = pd.read_csv(file_path, index_col=0)
    DAG = causal_dag.DAG.from_dataframe(DAG_adj_matrix)
    CPDAG = DAG.cpdag()

    return CPDAG

def allDAGSFromCPDAG(CPDAG):
    """
    Enumerate all DAGs in the I-equivalence class of a CPDAG.

    Args:
        CPDAG (causaldag.DAG): A CPDAG causaldag object

    Returns:
        list of causaldag.DAG: A list of DAG objects that are consistent with the input CPDAG.
    """
    all_DAGs = CPDAG.all_dags(verbose=True)
    return all_DAGs

def save_DBN_VE_engine(dbn_model, filename="DBN_VE_MLE_unrolled.pkl"):
    """
    Initializes a Variable Elimination object for a given unrolled Dynamic Bayesian Network (DBN)
    and saves it to the data/final_network directory.

    Args:
        dbn_model (BayesianNetwork): The unrolled static Bayesian Network representing the DBN.
        filename (str): Name of the file to save the VariableElimination object. Defaults to 'DBN_VE_MLE_unrolled.pkl'.

    Returns:
        VariableElimination: The initialized Variable Elimination object.
    """
    base_dir = os.path.dirname(__file__)
    save_path = os.path.abspath(os.path.join(base_dir, "..", "data", "inference_engine", filename))

    VE_engine = VariableElimination(dbn_model)

    with open(save_path, 'wb') as f:
        pickle.dump(VE_engine, f)

    return VE_engine


def load_DBN_VE_engine(filename="DBN_VE_MLE_unrolled.pkl"):
    """
    Loads a saved Variable Elimination object from the data/final_network directory.

    Args:
        filename (str): Name of the file to load. Defaults to 'DBN_VE_MLE_unrolled.pkl'.

    Returns:
        VariableElimination: The loaded Variable Elimination object.
    """
    base_dir = os.path.dirname(__file__)
    load_path = os.path.abspath(os.path.join(base_dir, "..", "data", "inference_engine", filename))

    with open(load_path, 'rb') as f:
        VE_engine = pickle.load(f)

    return VE_engine

def saveIEquivalentDBNs(all_dbns, data, parameter_estimation="MLE", save_path=None):
    """
    Fits and saves all DBNs from a set of I-equivalent DAGs.

    Each DBN is constructed by unrolling a frozen edge set into a static Bayesian Network
    over 3 time slices, then fitted using the specified parameter estimation method.

    Args:
        all_dbns (set of frozensets): A set of frozen edge sets representing I-equivalent DAGs.
        data (pd.DataFrame): The dataset used for CPD parameter estimation.
        parameter_estimation (str): Parameter estimation method ('MLE' or 'Bayesian' supported). Default is 'MLE'.
        save_path (str, optional): Directory to save the fitted DBNs. Defaults to
            '../data/i_equivalent_dags/'

    Returns:
        None
    """
    if save_path is None:
        base_dir = os.path.dirname(__file__)
        save_path = os.path.abspath(os.path.join(base_dir, "..", "data", "i_equivalent_dags"))

    os.makedirs(save_path, exist_ok=True)

    all_dbns = list(all_dbns)
    print(f"Found {len(all_dbns)} DBNs. Fitting parameters and saving...")

    for i, dbn_edges in enumerate(all_dbns):
        dbn_edges_list = list(dbn_edges)
        filename = f"fitted_dbn_{i}_{parameter_estimation}.pkl"
        file_path = os.path.join(save_path, filename)

        # Build and fit the unrolled DBN from the edge set
        dbn_dag = buildUnrolledDBN(data, parameter_estimation, frozen_set=dbn_edges_list)

        # Save the fitted model to disk
        with open(file_path, 'wb') as f:
            pickle.dump(dbn_dag, f)

def loadIEquivalentDBNs(parameter_estimation="MLE", load_file_path=None):
    """
    Load all fitted I-equivalent DBNs saved location.

    Searches the specified directory for pickled unrolled DBNs fitted using a given parameter estimation method
    and loads them into memory. The filenames must follow the pattern: 'fitted_dbn_{i}_{method}.pkl'.

    Args:
        parameter_estimation (str): The parameter estimation method used to fit the DBNs
            (e.g., 'MLE' or 'Bayesian'). Default is 'MLE'.
        load_file_path (str, optional): Folder where the fitted DBNs are stored.
            If not provided, defaults to '../data/i_equivalent_dags/'

    Returns:
        dict: A dictionary mapping DBN index numbers (int) to their corresponding
              pgmpy BayesianNetwork objects.
    """
    if load_file_path is None:
        base_dir = os.path.dirname(__file__)
        dbn_folder = os.path.abspath(os.path.join(base_dir, "..", "data", "i_equivalent_dags"))
    else:
        dbn_folder = load_file_path

    all_fitted_dbns = [
        f for f in os.listdir(dbn_folder) if f.endswith(f"_{parameter_estimation}.pkl")
    ]

    if len(all_fitted_dbns) == 0:
        raise FileNotFoundError(f"No fitted DBNs found in {dbn_folder} for method {parameter_estimation}")

    DBNs = {}
    for filename in all_fitted_dbns:
        file_path = os.path.join(dbn_folder, filename)
        dag_number_match = re.search(r"fitted_dbn_(\d+)_", filename)
        dag_number = int(dag_number_match.group(1)) if dag_number_match else None

        if dag_number is None:
            print(f"Warning: Could not extract DAG number from {filename}. Skipping.")
            continue

        with open(file_path, "rb") as f:
            DBNs[dag_number] = pickle.load(f)

    print(f"Loaded all {len(DBNs)} DBNs for causal inference.")
    return DBNs

def probabilisticInference(VE_engine, target_nodes, evidence_dict=None,
                                    elimination_order='greedy', joint=True):
    """
    Perform probabilistic inference on a Bayesian network using variable elimination.

    Supports standard and multi-level (OR condition) evidence and can return either
    a joint distribution over target variables or separate marginals for each.

    Args:
        VE_engine (VariableElimination): A pgmpy inference engine initialized on a fitted BayesianNetwork.
        target_nodes (list): List of one or more target node names for which to compute the distribution.
        evidence_dict (dict, optional): Dictionary of observed evidence.
            Keys are variable names, values are observed states (single value or list for multi-state evidence).
        elimination_order (str): Strategy used for variable elimination (e.g., 'greedy', 'MinFill'). Default is 'greedy'.
        joint (bool): Whether to return a joint distribution (True) or separate marginals (False). Default is True.

    Returns:
        pd.DataFrame: A DataFrame with the columns:
            - 'target_node': Name(s) of the queried variable(s)
            - 'target_level': State or state combination being evaluated
            - 'target_probability': Probability of that configuration
            - 'evidence': String representation of the conditioning evidence
    """
    virtual_evidence = []

    # Process multi-level evidence (OR conditions) by creating virtual evidence CPDs
    if evidence_dict:
        updated_evidence = {}
        for var, value in evidence_dict.items():
            if isinstance(value, list):
                # Convert list-style evidence (e.g., var in ['1', '2']) to virtual evidence
                states = VE_engine.model.states[var]
                weights = [[1.0] if state in value else [0.0] for state in states]
                CPD = TabularCPD(variable=var,
                                 variable_card=len(states),
                                 values=weights,
                                 state_names={var: [state for state in states]}
                                 )
                virtual_evidence.append(CPD)
            else:
                # Single-state evidence remains the same
                updated_evidence[var] = value
    else:
        updated_evidence = None

    # Choose appropriate query parameters based on presence of standard vs virtual evidence or both
    if updated_evidence and virtual_evidence:
        result = VE_engine.query(
            variables=target_nodes,
            evidence=updated_evidence,
            virtual_evidence=virtual_evidence,
            joint=joint,
            elimination_order=elimination_order
        )
    elif updated_evidence and not virtual_evidence:
        result = VE_engine.query(
            variables=target_nodes,
            evidence=updated_evidence,
            joint=joint,
            elimination_order=elimination_order
        )
    elif virtual_evidence and not updated_evidence:
        result = VE_engine.query(
            variables=target_nodes,
            virtual_evidence=virtual_evidence,
            joint=joint,
            elimination_order=elimination_order
        )
    else:
        result = VE_engine.query(
            variables=target_nodes,
            joint=joint,
            elimination_order=elimination_order
        )

    reshaped_results = []

    if joint:
        if len(target_nodes) == 1:
            target_node = target_nodes[0]
            state_names = result.state_names[target_node]
            probabilities = result.values

            reshaped_results = pd.DataFrame({
                "target_node": target_node,
                "target_level": state_names,
                "target_probability": probabilities
            })
        else:
            # Multiple target nodes
            state_names = result.state_names
            probabilities = result.values
            level_combinations = list(product(*[state_names[node] for node in target_nodes]))
            flat_probabilities = probabilities.flatten()

            reshaped_results = pd.DataFrame({
                "target_node": ", ".join(target_nodes),
                "target_level": [", ".join(map(str, levels)) for levels in level_combinations],
                "target_probability": flat_probabilities,
            })
    else:
        for target, dist in result.items():
            state_names = dist.state_names[target]
            probabilities = dist.values

            result_df = pd.DataFrame({
                "target_node": target,
                "target_level": state_names,
                "target_probability": probabilities
            })
            reshaped_results.append(result_df)

        reshaped_results = pd.concat(reshaped_results, ignore_index=True)

    reshaped_results["evidence"] = str(evidence_dict) if evidence_dict else "None"

    return reshaped_results

def batchProbabilisticInference(VE_engine, target_nodes, evidence_list,
                                  elimination_order='greedy', joint=True, n_jobs=-1):
    """
    Perform parallel batch inference across multiple evidence combinations using joblib to run multiple
    inference queries in parallel.

    Args:
        VE_engine (VariableElimination): A pgmpy inference engine initialized on a fitted BayesianNetwork.
        target_nodes (list): List of one or more target node names for which to compute the distribution.
        evidence_list (list of dict): Each dictionary specifies a unique set of evidence for one query.
            Keys are variable names, values are observed states (single value or list for multi-state evidence).
        elimination_order (str): Strategy used for variable elimination (e.g., 'greedy', 'MinFill'). Default is 'greedy'.
        joint (bool): Whether to return a joint distribution (True) or separate marginals (False). Default is True.
        n_jobs (int): Number of parallel processes to use. Set to -1 to use all available cores.

    Returns:
        pd.DataFrame: Combined inference results for all evidence sets. Columns include:
            - 'target_node'
            - 'target_level'
            - 'target_probability'
            - 'evidence'
    """
    def single_inference(evidence_dict):
        local_inference_engine = VariableElimination(VE_engine.model)
        result_df = probabilisticInference(
            VE_engine=local_inference_engine,
            target_nodes=target_nodes,
            evidence_dict=evidence_dict,
            elimination_order=elimination_order,
            joint=joint
        )
        result_df['evidence'] = str(evidence_dict)
        return result_df

    results = Parallel(n_jobs=n_jobs)(delayed(single_inference)(evidence) for evidence in evidence_list)

    combined_results = pd.concat(results, ignore_index=True)

    return combined_results

def mutilateDBN(DBN, intervention_vars):
    """
    Create a mutilated Bayesian Network for causal inference by removing incoming edges to intervention variables.

    Simulates a do-intervention by removing all incoming edges to each specified intervention variable, and
    replacing its CPD with a uniform distribution across its states.

    Args:
        DBN (BayesianNetwork): A pgmpy Bayesian Network object (e.g., an unrolled DBN).
        intervention_vars (list of str): Names of variables to intervene on for causal inference.

    Returns:
        BayesianNetwork: The mutilated BayesianNetwork object.
    """
    # Create a copy of the original model to avoid modifying it
    mutilated_DBN = copy.deepcopy(DBN)

    # Iterate over each intervention variable
    for intervention_var in intervention_vars:
        parents = DBN.get_parents(intervention_var)
        # Remove all incoming edges to the intervention variable
        for parent in parents:
            mutilated_DBN.remove_edge(parent, intervention_var)

        # Update the CPD for the intervention variable to a uniform CPD
        originalCPD = DBN.get_cpds(intervention_var)
        state_names = originalCPD.state_names[intervention_var]
        num_states = originalCPD.variable_card

        newCPD_values = [[1 / num_states] for _ in range(num_states)]
        newCPD = TabularCPD(
            variable=intervention_var,
            variable_card=num_states,
            values=newCPD_values,
            state_names={intervention_var: state_names}
        )
        mutilated_DBN.add_cpds(newCPD)

    return mutilated_DBN

def causalInference(DBN, target_nodes, intervention_dict, evidence_dict=None, elimination_order='greedy', joint=True):
    """
    Implements interventional (causal) inference by constructing a mutilated Bayesian Network.
    Exact inference is then performed using variable elimination on the modified network.

    Args:
        DBN (BayesianNetwork): A pgmpy Bayesian Network (e.g., unrolled DBN) with estimated parameters.
        target_nodes (list): List of one or more target node names for which to compute the distribution.
        intervention_dict (dict): Dictionary specifying the intervention. Keys are variable names,
            values are fixed states.
        evidence_dict (dict, optional): Optional dictionary of observed evidence.
            Combined with the intervention for conditioning during inference.
        elimination_order (str): Strategy used for variable elimination (e.g., 'greedy', 'MinFill'). Default is 'greedy'.
        joint (bool): Whether to return a joint distribution (True) or separate marginals (False). Default is True.

    Returns:
        pd.DataFrame: A DataFrame with the columns:
            - 'target_node': Name(s) of the queried variable(s)
            - 'target_level': State or state combination being evaluated
            - 'target_probability': Probability of that configuration
            - 'evidence': String representation of both intervention and observation
    """
    # Step 1: Create the mutilated DBN
    intervention_vars = list(intervention_dict.keys())
    mutilated_model = mutilateDBN(DBN, intervention_vars)

    # Step 2: Initialize Variable Elimination object on the mutilated DBN
    inference_engine = VariableElimination(mutilated_model)

    # Step 3: Combine intervention and evidence for inference
    combined_evidence = {**intervention_dict, **(evidence_dict or {})}

    # Step 4: Perform probabilistic inference on the mutilated DBN
    results = probabilisticInference(
        VE_engine=inference_engine,
        target_nodes=target_nodes,
        evidence_dict=combined_evidence,
        elimination_order=elimination_order,
        joint=joint
    )

    return results

def causalInferenceAllDAGs(target_nodes, intervention_dict, evidence_dict=None,
                                   elimination_order='greedy', joint=True, parameter_method="MLE"):
    """
    Perform causal inference across all sampled I-equivalent DAGs.

    Loads a set of pre-fitted unrolled Bayesian Networksâ€”each representing a
    DAG from the I-equivalence class and performs causal inference on each one.

    Args:
        target_nodes (list): List of one or more target node names for which to compute the distribution.
        intervention_dict (dict): Dictionary specifying the intervention. Keys are variable names,
            values are fixed states.
        evidence_dict (dict, optional): Optional dictionary of observed evidence.
            Combined with the intervention for conditioning during inference.
        elimination_order (str): Strategy used for variable elimination (e.g., 'greedy', 'MinFill'). Default is 'greedy'.
        joint (bool): Whether to return a joint distribution (True) or separate marginals (False). Default is True.
        parameter_method (str): Specifies which set of fitted DBNs to load (e.g., 'MLE' or 'Bayesian').

    Returns:
        pd.DataFrame: Combined inference results across all DAGs, with columns:
            - 'target_node'
            - 'target_level'
            - 'target_probability'
            - 'evidence'
            - 'intervention'
            - 'dag_number'
    """

    DBNs = loadIEquivalentDBNs(parameter_method)

    all_results = []

    for dbn_number, dbn_model in DBNs.items():
        result = causalInference(
            DBN=dbn_model,
            target_nodes=target_nodes,
            intervention_dict=intervention_dict,
            evidence_dict=evidence_dict,
            elimination_order=elimination_order,
            joint=joint
        )
        result["dag_number"] = dbn_number
        result["intervention"] = str(intervention_dict) if intervention_dict else "None"
        result["evidence"] = str(evidence_dict) if evidence_dict else "None"

        if joint and len(target_nodes) > 1:
            target_levels = result["target_level"].apply(lambda x: tuple(x.split(", ")))
            result["target_level"] = target_levels

        all_results.append(result)

    allResultDf = pd.concat(all_results, ignore_index=True)
    return allResultDf

def batchCausalInferenceAllDAGs(target_nodes, scenario_list,
                                elimination_order='greedy', joint=True, parameter_method="MLE"):
    """
    Wrapper function to perform causal inference for multiple intervention/evidence scenarios
    across all I-equivalent DAGs.

    Args:
        target_nodes (list): List of one or more target node names for which to compute the distribution.
        scenario_list (list of dict): Each element is a dictionary with:
            - intervention_dict: Dictionary of intervention variables and their fixed values.
            - evidence_dict: Dictionary of observed evidence for inference (optional).
        elimination_order (str): Strategy used for variable elimination (e.g., 'greedy', 'MinFill'). Default is 'greedy'.
        joint (bool): Whether to return a joint distribution (True) or separate marginals (False). Default is True.
        parameter_method (str): Specifies which set of fitted DBNs to load (e.g., 'MLE' or 'Bayesian').

    Returns:
        pd.DataFrame: Aggregated results from all scenarios and sampled DAGs.
    """

    all_results = []

    for scenario in scenario_list:
        result = causalInferenceAllDAGs(
            target_nodes=target_nodes,
            intervention_dict=scenario.get("intervention_dict", {}),
            evidence_dict=scenario.get("evidence_dict", {}),
            elimination_order=elimination_order,
            joint=joint,
            parameter_method=parameter_method
        )
        all_results.append(result)

    allResultDf = pd.concat(all_results, ignore_index=True)
    return allResultDf
